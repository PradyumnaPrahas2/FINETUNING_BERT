{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9560805,"sourceType":"datasetVersion","datasetId":5826270},{"sourceId":9584174,"sourceType":"datasetVersion","datasetId":5844432}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## *The first 13 Cells are for clear understanding of BERT Architecture, if you want to skip it then take a long jump to cell 14*","metadata":{"execution":{"iopub.status.busy":"2024-11-26T13:31:43.677393Z","iopub.execute_input":"2024-11-26T13:31:43.677953Z","iopub.status.idle":"2024-11-26T13:31:43.705169Z","shell.execute_reply.started":"2024-11-26T13:31:43.677906Z","shell.execute_reply":"2024-11-26T13:31:43.703698Z"}}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize a sample sentence\nsentence = [\n    \"BERT is a transformer model developed by Google. It has revolutionized NLP by introducing bidirectional context and allowing pre-training on vast datasets.\",\n]\ntokens = tokenizer.tokenize(sentence[0])\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(\"Tokens:\", tokens)\nprint(\"Token IDs:\", token_ids)\nprint(len(token_ids))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:08:38.147114Z","iopub.execute_input":"2024-10-25T11:08:38.147446Z","iopub.status.idle":"2024-10-25T11:08:38.297761Z","shell.execute_reply.started":"2024-10-25T11:08:38.147405Z","shell.execute_reply":"2024-10-25T11:08:38.296849Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Tokens: ['bert', 'is', 'a', 'transform', '##er', 'model', 'developed', 'by', 'google', '.', 'it', 'has', 'revolution', '##ized', 'nl', '##p', 'by', 'introducing', 'bid', '##ire', '##ction', '##al', 'context', 'and', 'allowing', 'pre', '-', 'training', 'on', 'vast', 'data', '##set', '##s', '.']\nToken IDs: [14324, 2003, 1037, 10938, 2121, 2944, 2764, 2011, 8224, 1012, 2009, 2038, 4329, 3550, 17953, 2361, 2011, 10449, 7226, 7442, 7542, 2389, 6123, 1998, 4352, 3653, 1011, 2731, 2006, 6565, 2951, 13462, 2015, 1012]\n34\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Tokenizer INFO","metadata":{}},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:41:45.659977Z","iopub.execute_input":"2024-10-24T09:41:45.660757Z","iopub.status.idle":"2024-10-24T09:41:45.666466Z","shell.execute_reply.started":"2024-10-24T09:41:45.660722Z","shell.execute_reply":"2024-10-24T09:41:45.665559Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"len(tokenizer.vocab), type(tokenizer.vocab)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:38:51.973773Z","iopub.execute_input":"2024-10-24T08:38:51.974693Z","iopub.status.idle":"2024-10-24T08:38:51.980864Z","shell.execute_reply.started":"2024-10-24T08:38:51.974653Z","shell.execute_reply":"2024-10-24T08:38:51.979996Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(30522, collections.OrderedDict)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"for token, index in tokenizer.vocab.items():\n    print(f\"Index {index}: '{token}'\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Token Embeddings","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntokens = tokenizer.tokenize(\"I am pradyumna prahas\")\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(token_ids)  ","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:42:21.291936Z","iopub.execute_input":"2024-10-24T09:42:21.292347Z","iopub.status.idle":"2024-10-24T09:42:21.562916Z","shell.execute_reply.started":"2024-10-24T09:42:21.292305Z","shell.execute_reply":"2024-10-24T09:42:21.561897Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[1045, 2572, 10975, 18632, 2819, 2532, 10975, 23278, 2015]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:42:32.698120Z","iopub.execute_input":"2024-10-24T09:42:32.698498Z","iopub.status.idle":"2024-10-24T09:42:32.705702Z","shell.execute_reply.started":"2024-10-24T09:42:32.698461Z","shell.execute_reply":"2024-10-24T09:42:32.704466Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"#### Segment Embeddings","metadata":{}},{"cell_type":"code","source":"import torch\n\nsegment_a = torch.zeros(len(token_ids))  \nsegment_b = torch.ones(len(token_ids))    \n\nprint(\"Segment A:\", segment_a)\nprint(\"Segment B:\", segment_b)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:42:53.697223Z","iopub.execute_input":"2024-10-24T09:42:53.697629Z","iopub.status.idle":"2024-10-24T09:42:53.790307Z","shell.execute_reply.started":"2024-10-24T09:42:53.697581Z","shell.execute_reply":"2024-10-24T09:42:53.789332Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Segment A: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.])\nSegment B: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"#### Positional Embeddings","metadata":{}},{"cell_type":"code","source":"\nmax_position_embeddings = 512\npositional_embeddings = torch.arange(max_position_embeddings).unsqueeze(0)\n\nprint(\"Positional Embeddings Shape:\", positional_embeddings.shape)  \npositional_embeddings","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:42:58.831679Z","iopub.execute_input":"2024-10-24T09:42:58.832041Z","iopub.status.idle":"2024-10-24T09:42:58.851950Z","shell.execute_reply.started":"2024-10-24T09:42:58.832009Z","shell.execute_reply":"2024-10-24T09:42:58.851045Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Positional Embeddings Shape: torch.Size([1, 512])\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n         168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n         182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n         196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n         210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n         224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n         252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n         266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n         280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n         294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n         378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n         392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n         434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n         462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n         476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n         490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n         504, 505, 506, 507, 508, 509, 510, 511]])"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### Transformer layer","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\nimport torch\n\n# Initialize the BERT model\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Prepare input tensors\ninputs = tokenizer(\"Hello, BERT! How are you?\", return_tensors='pt')\noutputs = model(**inputs)\n\n# Output from the last layer\nlast_hidden_states = outputs.last_hidden_state\nprint(\"Last Hidden States Shape:\", last_hidden_states.shape)  ","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:41:20.454224Z","iopub.execute_input":"2024-10-24T08:41:20.454752Z","iopub.status.idle":"2024-10-24T08:41:20.914710Z","shell.execute_reply.started":"2024-10-24T08:41:20.454712Z","shell.execute_reply":"2024-10-24T08:41:20.913695Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Last Hidden States Shape: torch.Size([1, 10, 768])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch.nn as nn\n\n# Example feed-forward layer (as used in BERT)\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, hidden_size, intermediate_size):\n        super(FeedForwardNetwork, self).__init__()\n        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.activation(self.fc1(x)))\n\n# Initialize the feed-forward network\nffn = FeedForwardNetwork(hidden_size=768, intermediate_size=3072)\nsample_input = torch.rand(2, 10, 768)  # Batch size of 2, sequence length of 10\noutput = ffn(sample_input)\n\nprint(\"Feed Forward Output Shape:\", output.shape)  # (2, 10, 768)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:41:25.793570Z","iopub.execute_input":"2024-10-24T08:41:25.794475Z","iopub.status.idle":"2024-10-24T08:41:25.854344Z","shell.execute_reply.started":"2024-10-24T08:41:25.794425Z","shell.execute_reply":"2024-10-24T08:41:25.853399Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Feed Forward Output Shape: torch.Size([2, 10, 768])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class TransformerLayer(nn.Module):\n    def __init__(self, hidden_size, intermediate_size):\n        super(TransformerLayer, self).__init__()\n        self.attention = nn.MultiheadAttention(hidden_size, num_heads=12)\n        self.ffn = FeedForwardNetwork(hidden_size, intermediate_size)\n        self.norm1 = nn.LayerNorm(hidden_size)\n        self.norm2 = nn.LayerNorm(hidden_size)\n\n    def forward(self, x):\n        \n        attn_output, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_output)  # Residual connection + normalization\n\n        # Feed-forward network with residual connection\n        ffn_output = self.ffn(x)\n        return self.norm2(x + ffn_output)  # Residual connection + normalization\n\n# Initialize the transformer layer\ntransformer_layer = TransformerLayer(hidden_size=768, intermediate_size=3072)\nsample_transformer_input = torch.rand(2, 10, 768)  # Batch size of 2, sequence length of 10\ntransformer_output = transformer_layer(sample_transformer_input)\n\nprint(\"Transformer Layer Output Shape:\", transformer_output.shape)  # (2, 10, 768)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:41:41.568829Z","iopub.execute_input":"2024-10-24T08:41:41.569696Z","iopub.status.idle":"2024-10-24T08:41:41.649100Z","shell.execute_reply.started":"2024-10-24T08:41:41.569654Z","shell.execute_reply":"2024-10-24T08:41:41.648127Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Transformer Layer Output Shape: torch.Size([2, 10, 768])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### OUTPUT LAYER","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\n# Load the BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Forward pass for a sample input\noutputs = model(**inputs)\nlogits = outputs.logits\nprint(\"Logits Shape:\", logits.shape)  # (batch_size, num_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:41:45.603331Z","iopub.execute_input":"2024-10-24T08:41:45.603729Z","iopub.status.idle":"2024-10-24T08:41:45.922976Z","shell.execute_reply.started":"2024-10-24T08:41:45.603690Z","shell.execute_reply":"2024-10-24T08:41:45.922012Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Logits Shape: torch.Size([1, 2])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"1. USING THE PRE_TRAINED CLASSIFICATION MODEL","metadata":{}},{"cell_type":"markdown","source":"### BERT FINETUNING","metadata":{}},{"cell_type":"code","source":"import numpy as np,pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-20T10:31:20.605939Z","iopub.execute_input":"2024-11-20T10:31:20.606359Z","iopub.status.idle":"2024-11-20T10:31:20.959300Z","shell.execute_reply.started":"2024-11-20T10:31:20.606332Z","shell.execute_reply":"2024-11-20T10:31:20.958388Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import requests\nimport json\nimport torch\nimport os\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-11-25T12:13:51.461691Z","iopub.execute_input":"2024-11-25T12:13:51.462061Z","iopub.status.idle":"2024-11-25T12:13:51.466484Z","shell.execute_reply.started":"2024-11-25T12:13:51.462029Z","shell.execute_reply":"2024-11-25T12:13:51.465377Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\n\ndirectory_path = './BERT-SQuAD'  \n\nif not os.path.exists(directory_path):\n    os.mkdir(directory_path)\n\nprint(f\"Directory '{directory_path}' created.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:02:07.873922Z","iopub.execute_input":"2024-10-06T07:02:07.875046Z","iopub.status.idle":"2024-10-06T07:02:07.881147Z","shell.execute_reply.started":"2024-10-06T07:02:07.875002Z","shell.execute_reply":"2024-10-06T07:02:07.880123Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Directory './BERT-SQuAD' created.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:45:50.910278Z","iopub.execute_input":"2024-11-20T08:45:50.910996Z","iopub.status.idle":"2024-11-20T08:46:00.016223Z","shell.execute_reply.started":"2024-11-20T08:45:50.910948Z","shell.execute_reply":"2024-11-20T08:46:00.015232Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Squad Dataset Download","metadata":{}},{"cell_type":"code","source":"!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:31:33.817581Z","iopub.execute_input":"2024-11-20T10:31:33.818426Z","iopub.status.idle":"2024-11-20T10:31:36.611512Z","shell.execute_reply.started":"2024-11-20T10:31:33.818393Z","shell.execute_reply":"2024-11-20T10:31:36.610394Z"},"trusted":true},"outputs":[{"name":"stdout","text":"--2024-11-20 10:31:34--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.110.153, 185.199.111.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 42123633 (40M) [application/json]\nSaving to: 'train-v2.0.json'\n\ntrain-v2.0.json     100%[===================>]  40.17M  --.-KB/s    in 0.1s    \n\n2024-11-20 10:31:35 (313 MB/s) - 'train-v2.0.json' saved [42123633/42123633]\n\n--2024-11-20 10:31:36--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.111.153, 185.199.108.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4370528 (4.2M) [application/json]\nSaving to: 'dev-v2.0.json'\n\ndev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.06s   \n\n2024-11-20 10:31:36 (74.6 MB/s) - 'dev-v2.0.json' saved [4370528/4370528]\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Load the training dataset and take a look at it\nwith open('train-v2.0.json', 'rb') as f:\n  squad = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:31:39.849687Z","iopub.execute_input":"2024-11-20T10:31:39.850561Z","iopub.status.idle":"2024-11-20T10:31:40.718103Z","shell.execute_reply.started":"2024-11-20T10:31:39.850522Z","shell.execute_reply":"2024-11-20T10:31:40.717361Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"squad['data'][0].keys()","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:31:43.541985Z","iopub.execute_input":"2024-11-20T10:31:43.542649Z","iopub.status.idle":"2024-11-20T10:31:43.550111Z","shell.execute_reply.started":"2024-11-20T10:31:43.542599Z","shell.execute_reply":"2024-11-20T10:31:43.548829Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"dict_keys(['title', 'paragraphs'])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:31:46.199221Z","iopub.execute_input":"2024-11-20T10:31:46.199659Z","iopub.status.idle":"2024-11-20T10:31:46.204164Z","shell.execute_reply.started":"2024-11-20T10:31:46.199609Z","shell.execute_reply":"2024-11-20T10:31:46.203119Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"pprint(squad['data'][1][\"title\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:31:48.630019Z","iopub.execute_input":"2024-11-20T10:31:48.630353Z","iopub.status.idle":"2024-11-20T10:31:48.635029Z","shell.execute_reply.started":"2024-11-20T10:31:48.630325Z","shell.execute_reply":"2024-11-20T10:31:48.634055Z"},"trusted":true},"outputs":[{"name":"stdout","text":"'Frédéric_Chopin'\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"len(squad[\"data\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:09:19.322850Z","iopub.execute_input":"2024-11-20T10:09:19.323186Z","iopub.status.idle":"2024-11-20T10:09:19.328881Z","shell.execute_reply.started":"2024-11-20T10:09:19.323158Z","shell.execute_reply":"2024-11-20T10:09:19.328017Z"},"trusted":true},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"442"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"gr = -1\nfor idx, group in enumerate(squad['data']):\n  print(group['title'])\n  if group['title'] == 'Greece':\n    gr = idx\n    print(gr)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:02:42.015995Z","iopub.execute_input":"2024-10-06T07:02:42.016866Z","iopub.status.idle":"2024-10-06T07:02:42.024658Z","shell.execute_reply.started":"2024-10-06T07:02:42.016827Z","shell.execute_reply":"2024-10-06T07:02:42.023667Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Beyoncé\nFrédéric_Chopin\nSino-Tibetan_relations_during_the_Ming_dynasty\nIPod\nThe_Legend_of_Zelda:_Twilight_Princess\nSpectre_(2015_film)\n2008_Sichuan_earthquake\nNew_York_City\nTo_Kill_a_Mockingbird\nSolar_energy\nKanye_West\nBuddhism\nAmerican_Idol\nDog\n2008_Summer_Olympics_torch_relay\nGenome\nComprehensive_school\nRepublic_of_the_Congo\nPrime_minister\nInstitute_of_technology\nWayback_Machine\nDutch_Republic\nSymbiosis\nCanadian_Armed_Forces\nCardinal_(Catholicism)\nIranian_languages\nLighting\nSeparation_of_powers_under_the_United_States_Constitution\nArchitecture\nHuman_Development_Index\nSouthern_Europe\nBBC_Television\nArnold_Schwarzenegger\nPlymouth\nHeresy\nWarsaw_Pact\nMaterialism\nChristian\nSony_Music_Entertainment\nOklahoma_City\nHunter-gatherer\nUnited_Nations_Population_Fund\nRussian_Soviet_Federative_Socialist_Republic\nAlexander_Graham_Bell\nPub\nInternet_service_provider\nComics\nSaint_Helena\nAspirated_consonant\nHydrogen\nSpace_Race\nWeb_browser\nBeiDou_Navigation_Satellite_System\nCanon_law\nCommunications_in_Somalia\nCatalan_language\nBoston\nUniversal_Studios\nEstonian_language\nPaper\nAdult_contemporary_music\nDaylight_saving_time\nRoyal_Institute_of_British_Architects\nNational_Archives_and_Records_Administration\nTristan_da_Cunha\nUniversity_of_Kansas\nNanjing\nArena_Football_League\nDialect\nBern\nWestminster_Abbey\nPolitical_corruption\nClassical_music\nSlavs\nSouthampton\nTreaty\nJosip_Broz_Tito\nMarshall_Islands\nSzlachta\nVirgil\nAlps\nGene\nGuinea-Bissau\nList_of_numbered_streets_in_Manhattan\nBrain\nNear_East\nZhejiang\nMinistry_of_Defence_(United_Kingdom)\nHigh-definition_television\nWood\nSomalis\nMiddle_Ages\nPhonology\nComputer\nBlack_people\nThe_Times\nNew_Delhi\nBird_migration\nAtlantic_City,_New_Jersey\nImmunology\nMP3\nHouse_music\nLetter_case\nChihuahua_(state)\nImamah_(Shia_doctrine)\nPitch_(music)\nEngland_national_football_team\nHouston\nCopper\nIdentity_(social_science)\nHimachal_Pradesh\nCommunication\nGrape\nComputer_security\nOrthodox_Judaism\nAnimal\nBeer\nRace_and_ethnicity_in_the_United_States_Census\nUnited_States_dollar\nImperial_College_London\nHanover\nEmotion\nEverton_F.C.\nOld_English\nAircraft_carrier\nFederal_Aviation_Administration\nLancashire\nMesozoic\nVideoconferencing\nGregorian_calendar\nXbox_360\nMilitary_history_of_the_United_States\nHard_rock\nGreat_Plains\nInfrared\nBiodiversity\nASCII\nDigestion\nGymnastics\nFC_Barcelona\nFederal_Bureau_of_Investigation\nMary_(mother_of_Jesus)\nMelbourne\nJohn,_King_of_England\nMacintosh\nAnti-aircraft_warfare\nSanskrit\nValencia\nGeneral_Electric\nUnited_States_Army\nFranco-Prussian_War\nAdolescence\nAntarctica\nEritrea\nUranium\nOrder_of_the_British_Empire\nCircadian_rhythm\nElizabeth_II\nSexual_orientation\nDell\nCapital_punishment_in_the_United_States\nAge_of_Enlightenment\nNintendo_Entertainment_System\nAthanasius_of_Alexandria\nSeattle\nMemory\nMultiracial_American\nAshkenazi_Jews\nPharmaceutical_industry\nUmayyad_Caliphate\nAsphalt\nQueen_Victoria\nFreemasonry\nIsrael\nHellenistic_period\nBill_%26_Melinda_Gates_Foundation\nMontevideo\nPoultry\nDutch_language\nBuckingham_Palace\nIncandescent_light_bulb\nArsenal_F.C.\nClothing\nChicago_Cubs\nKorean_War\nCopyright_infringement\nGreece\n186\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def read_data(path):  \n  with open(path, 'rb') as f:\n    squad = json.load(f)\n\n  contexts = []\n  questions = []\n  answers = []\n\n  for group in squad['data']:\n    for passage in group['paragraphs']:\n      context = passage['context']\n      for qa in passage['qas']:\n        question = qa['question']\n        for answer in qa['answers']:\n          contexts.append(context)\n          questions.append(question)\n          answers.append(answer)\n\n  return contexts, questions, answers","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:03:09.787626Z","iopub.execute_input":"2024-10-06T07:03:09.788102Z","iopub.status.idle":"2024-10-06T07:03:09.795816Z","shell.execute_reply.started":"2024-10-06T07:03:09.788030Z","shell.execute_reply":"2024-10-06T07:03:09.794518Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_contexts, train_questions, train_answers = read_data('train-v2.0.json')\nvalid_contexts, valid_questions, valid_answers = read_data('dev-v2.0.json')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:03:13.045001Z","iopub.execute_input":"2024-10-06T07:03:13.046031Z","iopub.status.idle":"2024-10-06T07:03:14.325376Z","shell.execute_reply.started":"2024-10-06T07:03:13.045980Z","shell.execute_reply":"2024-10-06T07:03:14.324551Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# print a random question and answer\nprint(f'There are {len(train_questions)} questions')\nprint(train_questions[-10000])\nprint(train_answers[-10000])","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:03:16.300918Z","iopub.execute_input":"2024-10-06T07:03:16.301912Z","iopub.status.idle":"2024-10-06T07:03:16.306666Z","shell.execute_reply.started":"2024-10-06T07:03:16.301847Z","shell.execute_reply":"2024-10-06T07:03:16.305803Z"},"trusted":true},"outputs":[{"name":"stdout","text":"There are 86821 questions\nWhat is a modern common occurence with antibiotics?\n{'text': 'resistance of bacteria', 'answer_start': 17}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def add_end_idx(answers, contexts):\n  for answer, context in zip(answers, contexts):\n    gold_text = answer['text']\n    start_idx = answer['answer_start']\n    end_idx = start_idx + len(gold_text)\n\n    if context[start_idx:end_idx] == gold_text:\n      answer['answer_end'] = end_idx\n    elif context[start_idx-1:end_idx-1] == gold_text:\n      answer['answer_start'] = start_idx - 1\n      answer['answer_end'] = end_idx - 1     \n    elif context[start_idx-2:end_idx-2] == gold_text:\n      answer['answer_start'] = start_idx - 2\n      answer['answer_end'] = end_idx - 2     \n\nadd_end_idx(train_answers, train_contexts)\nadd_end_idx(valid_answers, valid_contexts)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:03:22.200988Z","iopub.execute_input":"2024-10-06T07:03:22.201883Z","iopub.status.idle":"2024-10-06T07:03:22.295169Z","shell.execute_reply.started":"2024-10-06T07:03:22.201830Z","shell.execute_reply":"2024-10-06T07:03:22.294141Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# You can see that now we get the answer_end also\nprint(train_questions[-10000])\nprint(train_answers[-10000])","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:03:24.655413Z","iopub.execute_input":"2024-10-06T07:03:24.656321Z","iopub.status.idle":"2024-10-06T07:03:24.661035Z","shell.execute_reply.started":"2024-10-06T07:03:24.656281Z","shell.execute_reply":"2024-10-06T07:03:24.660091Z"},"trusted":true},"outputs":[{"name":"stdout","text":"What is a modern common occurence with antibiotics?\n{'text': 'resistance of bacteria', 'answer_start': 17, 'answer_end': 39}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\ntrain_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\nvalid_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:03:27.845019Z","iopub.execute_input":"2024-10-06T07:03:27.845548Z","iopub.status.idle":"2024-10-06T07:04:09.256666Z","shell.execute_reply.started":"2024-10-06T07:03:27.845490Z","shell.execute_reply":"2024-10-06T07:04:09.255628Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19179764b2a843ecbca40bc4bf4d9b63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72bbb04c438d47ecbe60ee2d4d3707ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0e809e9c8aa4e15a44caf77f4ce5ed1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61baecdd9a0941d5ace1e78c7bcaea34"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"train_encodings.keys()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:04:13.732208Z","iopub.execute_input":"2024-10-06T07:04:13.732771Z","iopub.status.idle":"2024-10-06T07:04:13.740328Z","shell.execute_reply.started":"2024-10-06T07:04:13.732732Z","shell.execute_reply":"2024-10-06T07:04:13.739374Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"no_of_encodings = len(train_encodings['input_ids'])\nprint(f'We have {no_of_encodings} context-question pairs')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:04:16.165004Z","iopub.execute_input":"2024-10-06T07:04:16.165416Z","iopub.status.idle":"2024-10-06T07:04:16.171001Z","shell.execute_reply.started":"2024-10-06T07:04:16.165381Z","shell.execute_reply":"2024-10-06T07:04:16.169925Z"},"trusted":true},"outputs":[{"name":"stdout","text":"We have 86821 context-question pairs\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"train_encodings['input_ids'][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:04:18.120233Z","iopub.execute_input":"2024-10-06T07:04:18.121106Z","iopub.status.idle":"2024-10-06T07:04:18.136092Z","shell.execute_reply.started":"2024-10-06T07:04:18.121065Z","shell.execute_reply":"2024-10-06T07:04:18.135129Z"},"trusted":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[101,\n 20773,\n 21025,\n 19358,\n 22815,\n 1011,\n 5708,\n 1006,\n 1013,\n 12170,\n 23432,\n 29715,\n 3501,\n 29678,\n 12325,\n 29685,\n 1013,\n 10506,\n 1011,\n 10930,\n 2078,\n 1011,\n 2360,\n 1007,\n 1006,\n 2141,\n 2244,\n 1018,\n 1010,\n 3261,\n 1007,\n 2003,\n 2019,\n 2137,\n 3220,\n 1010,\n 6009,\n 1010,\n 2501,\n 3135,\n 1998,\n 3883,\n 1012,\n 2141,\n 1998,\n 2992,\n 1999,\n 5395,\n 1010,\n 3146,\n 1010,\n 2016,\n 2864,\n 1999,\n 2536,\n 4823,\n 1998,\n 5613,\n 6479,\n 2004,\n 1037,\n 2775,\n 1010,\n 1998,\n 3123,\n 2000,\n 4476,\n 1999,\n 1996,\n 2397,\n 4134,\n 2004,\n 2599,\n 3220,\n 1997,\n 1054,\n 1004,\n 1038,\n 2611,\n 1011,\n 2177,\n 10461,\n 1005,\n 1055,\n 2775,\n 1012,\n 3266,\n 2011,\n 2014,\n 2269,\n 1010,\n 25436,\n 22815,\n 1010,\n 1996,\n 2177,\n 2150,\n 2028,\n 1997,\n 1996,\n 2088,\n 1005,\n 1055,\n 2190,\n 1011,\n 4855,\n 2611,\n 2967,\n 1997,\n 2035,\n 2051,\n 1012,\n 2037,\n 14221,\n 2387,\n 1996,\n 2713,\n 1997,\n 20773,\n 1005,\n 1055,\n 2834,\n 2201,\n 1010,\n 20754,\n 1999,\n 2293,\n 1006,\n 2494,\n 1007,\n 1010,\n 2029,\n 2511,\n 2014,\n 2004,\n 1037,\n 3948,\n 3063,\n 4969,\n 1010,\n 3687,\n 2274,\n 8922,\n 2982,\n 1998,\n 2956,\n 1996,\n 4908,\n 2980,\n 2531,\n 2193,\n 1011,\n 2028,\n 3895,\n 1000,\n 4689,\n 1999,\n 2293,\n 1000,\n 1998,\n 1000,\n 3336,\n 2879,\n 1000,\n 1012,\n 102,\n 2043,\n 2106,\n 20773,\n 2707,\n 3352,\n 2759,\n 1029,\n 102,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0]"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"tokenizer.decode(train_encodings['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:04:23.616850Z","iopub.execute_input":"2024-10-06T07:04:23.617653Z","iopub.status.idle":"2024-10-06T07:04:37.715975Z","shell.execute_reply.started":"2024-10-06T07:04:23.617613Z","shell.execute_reply":"2024-10-06T07:04:37.715022Z"},"trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'[CLS] beyonce giselle knowles - carter ( / biːˈjɒnseɪ / bee - yon - say ) ( born september 4, 1981 ) is an american singer, songwriter, record producer and actress. born and raised in houston, texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of r & b girl - group destiny \\' s child. managed by her father, mathew knowles, the group became one of the world \\' s best - selling girl groups of all time. their hiatus saw the release of beyonce \\' s debut album, dangerously in love ( 2003 ), which established her as a solo artist worldwide, earned five grammy awards and featured the billboard hot 100 number - one singles \" crazy in love \" and \" baby boy \". [SEP] when did beyonce start becoming popular? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"def add_token_positions(encodings, answers):\n  start_positions = []\n  end_positions = []\n  for i in range(len(answers)):\n    start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n    end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n\n    if start_positions[-1] is None:\n      start_positions[-1] = tokenizer.model_max_length\n    if end_positions[-1] is None:\n      end_positions[-1] = tokenizer.model_max_length\n\n  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(valid_encodings, valid_answers)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:04:40.740536Z","iopub.execute_input":"2024-10-06T07:04:40.741226Z","iopub.status.idle":"2024-10-06T07:04:41.111227Z","shell.execute_reply.started":"2024-10-06T07:04:40.741186Z","shell.execute_reply":"2024-10-06T07:04:41.110415Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"train_encodings['start_positions'][:10]","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:04:43.481973Z","iopub.execute_input":"2024-10-06T07:04:43.482429Z","iopub.status.idle":"2024-10-06T07:04:43.489003Z","shell.execute_reply.started":"2024-10-06T07:04:43.482394Z","shell.execute_reply":"2024-10-06T07:04:43.488090Z"},"trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[67, 55, 128, 47, 69, 81, 124, 91, 69, 72]"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"class SQuAD_Dataset(torch.utils.data.Dataset):\n  def __init__(self, encodings):\n    self.encodings = encodings\n  def __getitem__(self, idx):\n    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n  def __len__(self):\n    return len(self.encodings.input_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:04:56.334759Z","iopub.execute_input":"2024-10-06T07:04:56.335446Z","iopub.status.idle":"2024-10-06T07:04:56.341261Z","shell.execute_reply.started":"2024-10-06T07:04:56.335409Z","shell.execute_reply":"2024-10-06T07:04:56.340113Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_dataset = SQuAD_Dataset(train_encodings)\nvalid_dataset = SQuAD_Dataset(valid_encodings)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:04:58.558956Z","iopub.execute_input":"2024-10-06T07:04:58.559356Z","iopub.status.idle":"2024-10-06T07:04:58.563826Z","shell.execute_reply.started":"2024-10-06T07:04:58.559320Z","shell.execute_reply":"2024-10-06T07:04:58.562853Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Define the dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:05:01.699468Z","iopub.execute_input":"2024-10-06T07:05:01.699836Z","iopub.status.idle":"2024-10-06T07:05:01.705316Z","shell.execute_reply.started":"2024-10-06T07:05:01.699803Z","shell.execute_reply":"2024-10-06T07:05:01.704313Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering\n\nmodel = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:05:04.284595Z","iopub.execute_input":"2024-10-06T07:05:04.284970Z","iopub.status.idle":"2024-10-06T07:05:13.043890Z","shell.execute_reply.started":"2024-10-06T07:05:04.284937Z","shell.execute_reply":"2024-10-06T07:05:13.043102Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cb78ed649ab4e2b8ccff6f308a7cfd1"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Working on {device}')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:05:24.082314Z","iopub.execute_input":"2024-10-06T07:05:24.082956Z","iopub.status.idle":"2024-10-06T07:05:24.119514Z","shell.execute_reply.started":"2024-10-06T07:05:24.082921Z","shell.execute_reply":"2024-10-06T07:05:24.118295Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Working on cuda\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from transformers import AdamW\n\nN_EPOCHS = 5\noptim = AdamW(model.parameters(), lr=5e-5)\n\nmodel.to(device)\nmodel.train()\n\nfor epoch in range(N_EPOCHS):\n  loop = tqdm(train_loader, leave=True)\n  for batch in loop:\n    optim.zero_grad()\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    start_positions = batch['start_positions'].to(device)\n    end_positions = batch['end_positions'].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n    loss = outputs[0]\n    loss.backward()\n    optim.step()\n\n    loop.set_description(f'Epoch {epoch+1}')\n    loop.set_postfix(loss=loss.item())\nmodel.save_pretrained(f\"/epoch{epoch}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T07:09:47.722226Z","iopub.execute_input":"2024-10-06T07:09:47.723187Z","iopub.status.idle":"2024-10-06T13:47:36.296613Z","shell.execute_reply.started":"2024-10-06T07:09:47.723142Z","shell.execute_reply":"2024-10-06T13:47:36.295739Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1: 100%|██████████| 5427/5427 [1:19:24<00:00,  1.14it/s, loss=0.919]\nEpoch 2: 100%|██████████| 5427/5427 [1:19:34<00:00,  1.14it/s, loss=1.72] \nEpoch 3: 100%|██████████| 5427/5427 [1:19:36<00:00,  1.14it/s, loss=0.594]\nEpoch 4: 100%|██████████| 5427/5427 [1:19:36<00:00,  1.14it/s, loss=0.923] \nEpoch 5: 100%|██████████| 5427/5427 [1:19:33<00:00,  1.14it/s, loss=0.376] \n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"print(\"Done and it took 9 hours\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T13:49:12.205100Z","iopub.execute_input":"2024-10-06T13:49:12.206071Z","iopub.status.idle":"2024-10-06T13:49:12.210831Z","shell.execute_reply.started":"2024-10-06T13:49:12.206015Z","shell.execute_reply":"2024-10-06T13:49:12.209895Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Done and it took 9 hours\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"model_path = '/kaggle/working/BERT-SQuAD'\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T13:50:30.333570Z","iopub.execute_input":"2024-10-06T13:50:30.333972Z","iopub.status.idle":"2024-10-06T13:50:31.322877Z","shell.execute_reply.started":"2024-10-06T13:50:30.333935Z","shell.execute_reply":"2024-10-06T13:50:31.321918Z"},"trusted":true},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/BERT-SQuAD/tokenizer_config.json',\n '/kaggle/working/BERT-SQuAD/special_tokens_map.json',\n '/kaggle/working/BERT-SQuAD/vocab.txt',\n '/kaggle/working/BERT-SQuAD/added_tokens.json',\n '/kaggle/working/BERT-SQuAD/tokenizer.json')"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering, BertTokenizerFast\n\nmodel_path = '/kaggle/input/finetuned-llm-bert'#loading the saved BERT Model\nmodel = BertForQuestionAnswering.from_pretrained(model_path)\ntokenizer = BertTokenizerFast.from_pretrained(model_path)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'Working on {device}')\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T12:13:59.325153Z","iopub.execute_input":"2024-11-25T12:13:59.325939Z","iopub.status.idle":"2024-11-25T12:14:03.589861Z","shell.execute_reply.started":"2024-11-25T12:13:59.325905Z","shell.execute_reply":"2024-11-25T12:14:03.588907Z"}},"outputs":[{"name":"stdout","text":"Working on cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-11-25T12:14:03.591605Z","iopub.execute_input":"2024-11-25T12:14:03.592299Z","iopub.status.idle":"2024-11-25T12:14:03.599730Z","shell.execute_reply.started":"2024-11-25T12:14:03.592268Z","shell.execute_reply":"2024-11-25T12:14:03.598926Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def get_prediction(context, question):\n  inputs = tokenizer.encode_plus(question, context, return_tensors='pt').to(device)\n  outputs = model(**inputs)\n  \n  answer_start = torch.argmax(outputs[0])  \n  answer_end = torch.argmax(outputs[1]) + 1 \n  \n  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n  \n  return answer\n\ndef normalize_text(s):\n  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n  import string, re\n  def remove_articles(text):\n    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n    return re.sub(regex, \" \", text)\n  def white_space_fix(text):\n    return \" \".join(text.split())\n  def remove_punc(text):\n    exclude = set(string.punctuation)\n    return \"\".join(ch for ch in text if ch not in exclude)\n  def lower(text):\n    return text.lower()\n\n  return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef exact_match(prediction, truth):\n    return bool(normalize_text(prediction) == normalize_text(truth))\n\ndef compute_f1(prediction, truth):\n  pred_tokens = normalize_text(prediction).split()\n  truth_tokens = normalize_text(truth).split()\n  \n  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n    return int(pred_tokens == truth_tokens)\n  \n  common_tokens = set(pred_tokens) & set(truth_tokens)\n  \n  # if there are no common tokens then f1 = 0\n  if len(common_tokens) == 0:\n    return 0\n  \n  prec = len(common_tokens) / len(pred_tokens)\n  rec = len(common_tokens) / len(truth_tokens)\n  \n  return round(2 * (prec * rec) / (prec + rec), 2)\n  \ndef question_answer(context, question,answer):\n  prediction = get_prediction(context,question)\n#   em_score = exact_match(prediction, answer)\n#   f1_score = compute_f1(prediction, answer)\n#   return  prediction\n  print(f'Question: {question}')\n  print(f'Prediction: {prediction}')\n  print(f'True Answer: {answer}')\n#   print(f'Exact match: {em_score}')\n#   print(f'F1 score: {f1_score}\\n')","metadata":{"execution":{"iopub.status.busy":"2024-11-25T12:14:07.924212Z","iopub.execute_input":"2024-11-25T12:14:07.924598Z","iopub.status.idle":"2024-11-25T12:14:07.935464Z","shell.execute_reply.started":"2024-11-25T12:14:07.924566Z","shell.execute_reply":"2024-11-25T12:14:07.934592Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"## ON OUR DATASET \ncontexts = [\"The Earth orbits the Sun once every 365.25 days, which constitutes a year.\"]\n\nquestions = ['How long does it take for the Earth to orbit the Sun?']\n\nanswers = [\"365.35 days\"]\n\nfor question, answer in zip(questions, answers):\n  question_answer(contexts[0], question, answer)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T12:16:26.229439Z","iopub.execute_input":"2024-11-25T12:16:26.230050Z","iopub.status.idle":"2024-11-25T12:16:26.247700Z","shell.execute_reply.started":"2024-11-25T12:16:26.230017Z","shell.execute_reply":"2024-11-25T12:16:26.246948Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question: How long does it take for the Earth to orbit the Sun?\nPrediction: a year\nTrue Answer: significant risks\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"context = \"\"\"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, \n          songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing \n          and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. \n          Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. \n          Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, \n          earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\"\"\"\n\n\nquestions = [\"For whom the passage is talking about?\",\n             \"When did Beyonce born?\",\n             \"Where did Beyonce born?\",\n             \"What is Beyonce's nationality?\",\n             \"Who was the Destiny's group manager?\",\n             \"What name has the Beyoncé's debut album?\",\n             \"How many Grammy Awards did Beyonce earn?\",\n             \"When did the Beyoncé's debut album release?\",\n             \"Who was the lead singer of R&B girl-group Destiny's Child?\"]\n\nanswers = [\"Beyonce Giselle Knowles - Carter\", \"September 4, 1981\", \"Houston, Texas\", \n           \"American\", \"Mathew Knowles\", \"Dangerously in Love\", \"five\", \"2003\", \n           \"Beyonce Giselle Knowles - Carter\"]\n\nfor question, answer in zip(questions, answers):\n  question_answer(context, question, answer)","metadata":{"execution":{"iopub.status.busy":"2024-11-25T12:14:27.046867Z","iopub.execute_input":"2024-11-25T12:14:27.047252Z","iopub.status.idle":"2024-11-25T12:14:27.550786Z","shell.execute_reply.started":"2024-11-25T12:14:27.047222Z","shell.execute_reply":"2024-11-25T12:14:27.549958Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question: For whom the passage is talking about?\nPrediction: hiatus\nTrue Answer: Beyonce Giselle Knowles - Carter\nQuestion: When did Beyonce born?\nPrediction: september 4, 1981\nTrue Answer: September 4, 1981\nQuestion: Where did Beyonce born?\nPrediction: houston, texas\nTrue Answer: Houston, Texas\nQuestion: What is Beyonce's nationality?\nPrediction: american\nTrue Answer: American\nQuestion: Who was the Destiny's group manager?\nPrediction: mathew knowles\nTrue Answer: Mathew Knowles\nQuestion: What name has the Beyoncé's debut album?\nPrediction: \nTrue Answer: Dangerously in Love\nQuestion: How many Grammy Awards did Beyonce earn?\nPrediction: 2003\nTrue Answer: five\nQuestion: When did the Beyoncé's debut album release?\nPrediction: 2003\nTrue Answer: 2003\nQuestion: Who was the lead singer of R&B girl-group Destiny's Child?\nPrediction: hiatus\nTrue Answer: Beyonce Giselle Knowles - Carter\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"context = \"\"\"Athens is the capital and largest city of Greece. Athens dominates the Attica region and is one of the world's oldest cities, \n             with its recorded history spanning over 3,400 years and its earliest human presence starting somewhere between the 11th and 7th millennium BC.\n             Classical Athens was a powerful city-state. It was a center for the arts, learning and philosophy, and the home of Plato's Academy and Aristotle's Lyceum.\n             It is widely referred to as the cradle of Western civilization and the birthplace of democracy, largely because of its cultural and political impact on the European continent—particularly Ancient Rome.\n             In modern times, Athens is a large cosmopolitan metropolis and central to economic, financial, industrial, maritime, political and cultural life in Greece. \n             In 2021, Athens' urban area hosted more than three and a half million people, which is around 35% of the entire population of Greece.\n             Athens is a Beta global city according to the Globalization and World Cities Research Network, and is one of the biggest economic centers in Southeastern Europe. \n             It also has a large financial sector, and its port Piraeus is both the largest passenger port in Europe, and the second largest in the world.\"\"\"\n\nquestions = [\"Which is the largest city in Greece?\",\n             \"For what was the Athens center?\",\n             \"Which city was the home of Plato's Academy?\"]\n\nanswers = [\"Athens\", \"center for the arts, learning and philosophy\", \"Athens\"]\n\nfor question, answer in zip(questions, answers):\n  question_answer(context, question, answer)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:18:44.381597Z","iopub.execute_input":"2024-11-20T10:18:44.382325Z","iopub.status.idle":"2024-11-20T10:18:44.434986Z","shell.execute_reply.started":"2024-11-20T10:18:44.382290Z","shell.execute_reply":"2024-11-20T10:18:44.434135Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question: Which is the largest city in Greece?\nPrediction: athens\nTrue Answer: Athens\nQuestion: For what was the Athens center?\nPrediction: \nTrue Answer: center for the arts, learning and philosophy\nQuestion: Which city was the home of Plato's Academy?\nPrediction: athens\nTrue Answer: Athens\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"context = \"\"\"Angelos Poulis was born on 8 April 2001 in Nicosia, Cyprus. He is half Cypriot and half Greek. \n            He is currently studying at the Department of Informatics and Telecommunications of the University of Athens in Greece. \n            His scientific interests are in the broad field of Artificial Intelligence and he loves to train neural networks! \n            Okay, I'm Angelos and I'll stop talking about me right now.\"\"\"\n\nquestions = [\"When did Angelos born?\",\n             \"In what university is Angelos studying now?\",\n             \"What is Angelos' nationality?\",\n             \"What are his scientific interests?\",\n             \"What I will do right now?\"]\n\nanswers = [\"8 April 2001\", \"University of Athens\", \n           \"half Cypriot and half Greek\", \"Artificial Intelligence\", \n           \"stop talking about me\"]\n\nfor question, answer in zip(questions, answers):\n  question_answer(context, question, answer)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:18:51.060359Z","iopub.execute_input":"2024-11-20T10:18:51.061060Z","iopub.status.idle":"2024-11-20T10:18:51.118181Z","shell.execute_reply.started":"2024-11-20T10:18:51.061025Z","shell.execute_reply":"2024-11-20T10:18:51.117307Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Question: When did Angelos born?\nPrediction: 8 april 2001\nTrue Answer: 8 April 2001\nQuestion: In what university is Angelos studying now?\nPrediction: university of athens\nTrue Answer: University of Athens\nQuestion: What is Angelos' nationality?\nPrediction: half cypriot\nTrue Answer: half Cypriot and half Greek\nQuestion: What are his scientific interests?\nPrediction: artificial intelligence\nTrue Answer: Artificial Intelligence\nQuestion: What I will do right now?\nPrediction: \nTrue Answer: stop talking about me\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"## INTRODUCTION TO RAG","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:37:13.829134Z","iopub.execute_input":"2024-10-13T16:37:13.829523Z","iopub.status.idle":"2024-10-13T16:37:27.736125Z","shell.execute_reply.started":"2024-10-13T16:37:13.829487Z","shell.execute_reply":"2024-10-13T16:37:27.734922Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->faiss-cpu) (3.1.2)\nDownloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.9.0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import faiss\nimport pickle\nimport os\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-11-23T16:53:10.117224Z","iopub.execute_input":"2024-11-23T16:53:10.118119Z","iopub.status.idle":"2024-11-23T16:53:10.276592Z","shell.execute_reply.started":"2024-11-23T16:53:10.118069Z","shell.execute_reply":"2024-11-23T16:53:10.275489Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"],"ename":"ModuleNotFoundError","evalue":"No module named 'faiss'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"class RagModel:\n    def __init__(self,model):\n        self.sentencetransformer=model\n        self.sentences=[]\n        self.dimension=384\n        self.vectordb=faiss.IndexFlatL2(self.dimension)\n        self.display_image(\"/kaggle/input/rag-datasets/RAG_Datasets/Modelphoto.png\")\n    def display_image(self, image_path):\n        \"\"\"Load and display an image.\"\"\"\n        if os.path.exists(image_path):\n            img = mpimg.imread(image_path)\n            plt.figure(figsize=(3,2))\n            plt.imshow(img)\n            plt.axis('off')  # Hide axes\n            plt.show()\n        else:\n            print(f\"Image not found: {image_path}\")\n    def train(self,X_train,Y_train):\n        try:\n            if(len(X_train)!=len(Y_train)):\n                raise ValueError(\"Index Error\")\n            for i in range(len(X_train)):\n                s=\"\"\n                if(X_train[i]!=None):\n                    s+=Y_train[i]\n                else:\n                    s+=X_train[i]+' the answer is '+Y_train[i]\n                vector=self.sentencetransformer.encode([s])[0]\n                self.sentences.append(s)\n                vector=np.array([vector],dtype='float32')\n                self.vectordb.add(vector)\n        except Exception as e:\n            raise RuntimeError(f\"Error occured while training is {e}\")\n    def predict(self,question,neighbours):\n        converted_vector=self.sentencetransformer.encode([question])[0]\n        converted_vector=np.array([converted_vector],dtype='float32')\n        distance,indices=self.vectordb.search(converted_vector,neighbours)\n        results=[]\n        for i in indices[0]:\n            sen=self.sentences[i]\n            if(sen not in results):\n                results.append(sen)\n#         results=[self.sentences[i] for i in indices[0]]\n        return results\n    def save(self, model_path):\n        if not os.path.exists(model_path):\n            os.makedirs(model_path)\n        faiss_index_path = os.path.join(model_path, \"faiss_index.bin\")\n        faiss.write_index(self.vectordb, faiss_index_path)\n\n        sentences_path = os.path.join(model_path, \"sentences.pkl\")\n        with open(sentences_path, \"wb\") as f:\n            pickle.dump(self.sentences, f)\n\n        print(f\"Model saved to {model_path}\")\n\n    def load(self, model_path):\n        faiss_index_path = os.path.join(model_path, \"faiss_index.bin\")\n        self.vectordb = faiss.read_index(faiss_index_path)\n        sentences_path = os.path.join(model_path, \"sentences.pkl\")\n        with open(sentences_path, \"rb\") as f:\n            self.sentences = pickle.load(f)\n        print(f\"Model loaded from {model_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:37:37.822377Z","iopub.execute_input":"2024-10-13T16:37:37.823128Z","iopub.status.idle":"2024-10-13T16:37:37.838637Z","shell.execute_reply.started":"2024-10-13T16:37:37.823089Z","shell.execute_reply":"2024-10-13T16:37:37.837486Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:37:42.994988Z","iopub.execute_input":"2024-10-13T16:37:42.995362Z","iopub.status.idle":"2024-10-13T16:37:55.000717Z","shell.execute_reply.started":"2024-10-13T16:37:42.995330Z","shell.execute_reply":"2024-10-13T16:37:54.999741Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.2.0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import re,time\nfrom scipy.spatial import distance\nfrom sentence_transformers import SentenceTransformer\nVector_model = SentenceTransformer('all-MiniLM-L6-v2')","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:37:57.952444Z","iopub.execute_input":"2024-10-13T16:37:57.952851Z","iopub.status.idle":"2024-10-13T16:38:15.142852Z","shell.execute_reply.started":"2024-10-13T16:37:57.952808Z","shell.execute_reply":"2024-10-13T16:38:15.142040Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ec354aaea66437f96da3575248bcfd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee8839dcb58740c68d9cfccf6daa4b2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b02e6f2837d64d23914058a607f2ac83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f897f1d84cab4b4580e2a0fd3f12e1b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6839ba0dc01499988d277f15e7e6e76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a86527cbe744e30816a981e425d50b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7580560db9f45318f2923fcb244d27f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28676bda74744e0da7bee955c0e0d71c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127fea6202894588b7890c2e5472112d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f42425d52f43d1b3b6b71d2f76d8c7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb55df4409884a5ba78f98533fe33542"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"mymodel=RagModel(Vector_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:38:26.062801Z","iopub.execute_input":"2024-10-13T16:38:26.063485Z","iopub.status.idle":"2024-10-13T16:38:26.159727Z","shell.execute_reply.started":"2024-10-13T16:38:26.063445Z","shell.execute_reply":"2024-10-13T16:38:26.158321Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 300x200 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"mymodel.load(\"/kaggle/input/rag-datasets/RAG_Datasets\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:38:29.313564Z","iopub.execute_input":"2024-10-13T16:38:29.314172Z","iopub.status.idle":"2024-10-13T16:38:29.324774Z","shell.execute_reply.started":"2024-10-13T16:38:29.314133Z","shell.execute_reply":"2024-10-13T16:38:29.323755Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model loaded from /kaggle/input/rag-datasets/RAG_Datasets\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"question='What is threat?'","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:41:13.380603Z","iopub.execute_input":"2024-10-13T16:41:13.381239Z","iopub.status.idle":"2024-10-13T16:41:13.386232Z","shell.execute_reply.started":"2024-10-13T16:41:13.381181Z","shell.execute_reply":"2024-10-13T16:41:13.385138Z"},"trusted":true},"outputs":[],"execution_count":50},{"cell_type":"code","source":"context_list=mymodel.predict(question,1)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:39:55.243569Z","iopub.execute_input":"2024-10-13T16:39:55.244429Z","iopub.status.idle":"2024-10-13T16:39:55.276825Z","shell.execute_reply.started":"2024-10-13T16:39:55.244376Z","shell.execute_reply":"2024-10-13T16:39:55.275954Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"658b385dc37e4abba0b8638332a0ad6a"}},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"for neighbour in range(10,0,-1):\n    try:\n        context_list=mymodel.predict(question,neighbour)\n        context=\"\"\n        for i in context_list:\n            context+=i+\" \"\n        extracted_answer=question_answer(context,question,\"\")\n        break\n    except:\n        continue","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:41:15.998432Z","iopub.execute_input":"2024-10-13T16:41:15.999181Z","iopub.status.idle":"2024-10-13T16:41:16.040730Z","shell.execute_reply.started":"2024-10-13T16:41:15.999142Z","shell.execute_reply":"2024-10-13T16:41:16.040032Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81cbd34b3b0e4146902f4490e4ebc6c2"}},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"print(\"RAG Output:- \",context)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:42:34.496867Z","iopub.execute_input":"2024-10-13T16:42:34.497750Z","iopub.status.idle":"2024-10-13T16:42:34.502217Z","shell.execute_reply.started":"2024-10-13T16:42:34.497710Z","shell.execute_reply":"2024-10-13T16:42:34.501233Z"},"trusted":true},"outputs":[{"name":"stdout","text":"RAG Output:-  In cybersecurity terms, a threat assessment refers to an evaluation of the risks and potential threats to an organization.   \n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"To use my BERT model which is Finetuned on SQUAD Dataset go to this link\n-> https://huggingface.co/Pradyumna22/Finetuned-BERT-SQUAD?library=transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"To use my RAG Model for Cybersecurity Related Queries go to this link\n-> Use the RAG Code and download the faiss index from here => https://huggingface.co/Pradyumna22/RAG-model-for-Cyber-security/tree/main","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}